{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb8ebf6",
   "metadata": {},
   "source": [
    "# NOTE 1!\n",
    "\n",
    "The day2greg notebook is developed by https://github.com/gmerritt123 following the notebook developed by Jonny Williams. This notebook is a modification of gmerritt123 notebook to convert 360 and 365 day calendar netcdf to gregorian calendar. This notebook uses xarray instead of netCDF4 package to explore the NetCDF file. It also creates a new NetCDF file after the modification into Gregorian calendar. \n",
    "\n",
    "\n",
    "# NOTE 2!\n",
    "Please do well to visit their pages and assign due credit to **https://github.com/jonnyhtw/360day2greg/tree/main, https://eartharxiv.org/repository/view/5645/ and https://github.com/gmerritt123 ** . \n",
    "\n",
    "I also utilise **ChatGPT from OpenAI** for debugging errors and streamlining the codes to make it more user friendly.\n",
    "\n",
    "The notebook can work with **both single or multiple** files, the files must be present in the appropriate folders. **Don't mix** 360_day calendar files(eg. HadGEM or UKESM1 models) in the **same folder** as 365_day calendar files.\n",
    "\n",
    "Finally, the original code in this notebook from __gmerritt123__ are in Markdown hereafter to serve as a reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c152b",
   "metadata": {},
   "source": [
    "## day2greg\n",
    "\n",
    "This notebook implements the methodology dev'd by Jonny Williams to adjust 365 day and 360 day calendar climate datasets to standard gregorian. It improves on the original notebook by standardizing the routine, leveraging numpy's array functions for fast vectorized operations, and using far fewer dependencies.\n",
    "\n",
    "See \n",
    "- https://github.com/jonnyhtw/360day2greg/tree/main ; and\n",
    "- https://eartharxiv.org/repository/view/5645/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34462ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1efa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Commence from here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f8c7d",
   "metadata": {},
   "source": [
    "from netCDF4 import Dataset\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import calendar\n",
    "import xarray as xr\n",
    "\n",
    "os.chdir(r'D:\\Dufie_Enoch_CMIP6\\ALL_MERGED_HAS_FUT\\HadGEM3_GC31_LL_ssp245') #just my working dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92c33dc-e0b5-4ca7-b4f6-1abc8e62fdb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dced7b6",
   "metadata": {},
   "source": [
    "## Just two functions!\n",
    "\n",
    "One to get an array of indices where to perform the interpolated insertions, and another to do them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c10ddfb",
   "metadata": {},
   "source": [
    "def getGregorianSeeds(num_yrs,y0,day_cal):\n",
    "    ''' used to obtain array of random \"seeds\" for adding extra interpolated days within each year.\n",
    "    args:\n",
    "    num_yrs = total number of years\n",
    "    y0 = year zero (e.g. 1999)\n",
    "    day_cal = the number of days in the calendar you're using (e.g. 360 or 365)\n",
    "    returns: \n",
    "        list of indices at which to perform \"interpolated insertions\"\n",
    "    '''\n",
    "    assert day_cal <= 365, 'getSeeds only configured for adding interpolated days, day_cal must be 365 or less'\n",
    "    ed = (365 - day_cal) # how many extra days you need to add to a \"normal\" year\n",
    "    ins = [] #running tally of indices to flag where interpolation should occur\n",
    "    #starting in y0 for the spec'd number of years...\n",
    "    for i,y in enumerate(range(y0,y0+num_yrs,1)):\n",
    "        #check if leap year\n",
    "        if calendar.isleap(y):\n",
    "            b = ed+2 #b = number of divisions to apply to year\n",
    "        else:\n",
    "            b = ed+1\n",
    "        sr = np.linspace(0,day_cal,b) #makes \"bins\" within the year by which to sample random seeds\n",
    "        if len(sr) == 1: #special case just handling if you don't need to sample any extra days\n",
    "            s = day_cal\n",
    "        else:\n",
    "            s = sr[1]-sr[0] #size of each bin \n",
    "        rs = np.round(np.random.rand(b-1)*s) #makes b-1 random numbers between 0 and 1 multiplied by the bin size to get an \"offset\" from sr\n",
    "        seeds = (sr[:-1]+rs).astype(int) #adds the offsets to the divisions \n",
    "        ins.extend(seeds+day_cal*i) #extends the running tally by the seeds + the number of days elapsed up to the current year\n",
    "    return ins\n",
    "\n",
    "def getInterpolatedData(arr,ins):\n",
    "    '''Inserts interpolated data into existing array given list of indices where to insert. DOES NOT ALTER EXISTING ARRAY\n",
    "    If indice is in ins, an additional new value will be inserted set to \"halfway\" between the current value and the next value.  \n",
    "    args:\n",
    "        - arr: array of data to get extra days inserted to it, typically for transient grid will be of shape (time,col,row)\n",
    "        - ins: list or array of of indices at which to do the interpolated insertion\n",
    "    returns: new array of length len(arr) + len(ins) containing the interpolated values in the spec'd indices\n",
    "        '''\n",
    "    r = [] #new data\n",
    "    for i,a in enumerate(arr):\n",
    "        r.append(a)\n",
    "        if i in ins:\n",
    "\n",
    "            r.append((arr[i]+arr[i+1])/2)\n",
    "    return np.array(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c5028b",
   "metadata": {},
   "source": [
    "# example usage 1\n",
    "\n",
    "- UKESM1 model is 360 day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638b8fe3",
   "metadata": {},
   "source": [
    "#load net cdf of precip\n",
    "fn = r'Precip\\\\pr_day_bccaqv2_anusplin300_ukesm1_0_ll_historical_ssp585_r1i1p1f2_gn_19500101_21001230_sub.nc'\n",
    "d = Dataset(fn)\n",
    "print('dataset calendar:')\n",
    "print(d.variables['time'].calendar) #360 day.\n",
    "\n",
    "va = d.variables['pr'][:].transpose((2,0,1)) #transposes variable from shape (col,row,time) into array (time,col,row)\n",
    "ny = int(va.shape[0]/360) #number of years\n",
    "y0 = 1950 #starting year\n",
    "\n",
    "ins = getGregorianSeeds(num_yrs=ny,y0=1950,day_cal=360) #gets a list of indices in the entire dataset where to do the interpolated insertions\n",
    "# print(ins)\n",
    "out = getInterpolatedData(arr=va,ins=ins) #makes a new array with the interpolated arrays inserted\n",
    "\n",
    "#check\n",
    "print('number of original days:')\n",
    "print(len(va))\n",
    "print('number of inserted days:')\n",
    "print(len(ins))\n",
    "print('number of gregorian-based days from y0 for ny')\n",
    "import pandas as pd #using this only for convenience of checking dates\n",
    "print(len(pd.date_range(start='Jan 1, 1950',end='Dec 31 2100',freq='1D')))\n",
    "print('number of days in updated dataset:')\n",
    "print(out.shape[0])\n",
    "del d \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0bd3bc57-02ba-445f-b467-89f820580ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360_day\n"
     ]
    }
   ],
   "source": [
    "print(d.variables['time'].calendar) #360 day.\n",
    "calendar_attr = ds['time']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39811ad9",
   "metadata": {},
   "source": [
    "MODIFIED FROM HERE ONWARDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df776da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7204f39a",
   "metadata": {},
   "source": [
    "# ALTERNATIVE 1  (To example 1 for 360_day)  : LOOPING FOR MULTIPLE FILES AND preserving SOURCE dimension attributes to output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bfee81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74336693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF file saved: /media/peterrock/Volume/Dufie_Enoch_CMIP6/ALL_MERGED_HAS_FUT/HadGEM3_GC31_LL_ssp245/output/output_pr_pr_day_HadGEM3-GC31-LL_ssp126_HIST_merged.nc\n",
      "NetCDF file saved: /media/peterrock/Volume/Dufie_Enoch_CMIP6/ALL_MERGED_HAS_FUT/HadGEM3_GC31_LL_ssp245/output/output_tasmin_tasmin_day_HadGEM3-GC31-MM_ssp585_HIST_merged.nc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil  # Added import for shutil\n",
    "import numpy as np\n",
    "import calendar\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "#############################################################################################################\n",
    "def getGregorianSeeds(num_yrs,y0,day_cal):\n",
    "    ''' used to obtain array of random \"seeds\" for adding extra interpolated days within each year.\n",
    "    args:\n",
    "    num_yrs = total number of years\n",
    "    y0 = year zero (e.g. 1999)\n",
    "    day_cal = the number of days in the calendar you're using (e.g. 360 or 365)\n",
    "    returns: \n",
    "        list of indices at which to perform \"interpolated insertions\"\n",
    "    '''\n",
    "    assert day_cal <= 365, 'getSeeds only configured for adding interpolated days, day_cal must be 365 or less'\n",
    "    ed = (365 - day_cal) # how many extra days you need to add to a \"normal\" year\n",
    "    ins = [] #running tally of indices to flag where interpolation should occur\n",
    "    #starting in y0 for the spec'd number of years...\n",
    "    for i,y in enumerate(range(y0,y0+num_yrs,1)):\n",
    "        #check if leap year\n",
    "        if calendar.isleap(y):\n",
    "            b = ed+2 #b = number of divisions to apply to year\n",
    "        else:\n",
    "            b = ed+1\n",
    "        sr = np.linspace(0,day_cal,b) #makes \"bins\" within the year by which to sample random seeds\n",
    "        if len(sr) == 1: #special case just handling if you don't need to sample any extra days\n",
    "            s = day_cal\n",
    "        else:\n",
    "            s = sr[1]-sr[0] #size of each bin \n",
    "        rs = np.round(np.random.rand(b-1)*s) #makes b-1 random numbers between 0 and 1 multiplied by the bin size to get an \"offset\" from sr\n",
    "        seeds = (sr[:-1]+rs).astype(int) #adds the offsets to the divisions \n",
    "        ins.extend(seeds+day_cal*i) #extends the running tally by the seeds + the number of days elapsed up to the current year\n",
    "    return ins\n",
    "\n",
    "def getInterpolatedData(arr,ins):\n",
    "    '''Inserts interpolated data into existing array given list of indices where to insert. DOES NOT ALTER EXISTING ARRAY\n",
    "    If indice is in ins, an additional new value will be inserted set to \"halfway\" between the current value and the next value.  \n",
    "    args:\n",
    "        - arr: array of data to get extra days inserted to it, typically for transient grid will be of shape (time,col,row)\n",
    "        - ins: list or array of of indices at which to do the interpolated insertion\n",
    "    returns: new array of length len(arr) + len(ins) containing the interpolated values in the spec'd indices\n",
    "        '''\n",
    "    r = [] #new data\n",
    "    for i,a in enumerate(arr):\n",
    "        r.append(a)\n",
    "        if i in ins:\n",
    "\n",
    "            r.append((arr[i]+arr[i+1])/2)\n",
    "    return np.array(r)\n",
    "##################################################################################################################\n",
    "\n",
    "\n",
    "def process_file(file_path, variable_name):\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    #va = d.variables['pr'][:].transpose((2,0,1)) #transposes variable from shape (col,row,time) into array (time,col,row)\n",
    "    va = ds[variable_name].values # I did not transpose because my data had the shape array (time,col,row)\n",
    "    ny = int(va.shape[0] / 360) #number of years\n",
    "    y0 = 1960  #starting year\n",
    "\n",
    "    ins = getGregorianSeeds(num_yrs=ny, y0=1960, day_cal=360) #gets a list of indices in the entire dataset where to do the interpolated insertions\n",
    "    # print(ins)\n",
    "    out = getInterpolatedData(arr=va, ins=ins) #makes a new array with the interpolated arrays inserted\n",
    "    \n",
    "\n",
    "    # Define time coordinate and Copy Global attributes and variables definitions and attributes from source data\n",
    "    time_coords = pd.date_range(start='Jan 1, 1960', end='Dec 31 2100', freq='1D')\n",
    "    latitude_coords = ds['lat'].values\n",
    "    longitude_coords = ds['lon'].values\n",
    "\n",
    "    time_dim = xr.DataArray(time_coords, dims='time', name='time', attrs=ds['time'].attrs)\n",
    "    latitude_dim = xr.DataArray(latitude_coords, dims='lat', name='lat', attrs=ds['lat'].attrs)\n",
    "    longitude_dim = xr.DataArray(longitude_coords, dims='lon', name='lon', attrs=ds['lon'].attrs)\n",
    "\n",
    "    ### Creation and Definition of the structure of the netcdf files \n",
    "    pr_da = xr.DataArray(out, dims=('time', 'lat', 'lon'), coords={'time': time_dim, 'lat': latitude_dim, 'lon': longitude_dim}, attrs=ds[variable_name].attrs)\n",
    "\n",
    "    new_ds = xr.Dataset({variable_name: pr_da})\n",
    "\n",
    "    # Copy global attributes from the original dataset\n",
    "    for attr in ds.attrs:\n",
    "        new_ds.attrs[attr] = ds.attrs[attr]\n",
    "\n",
    "    # Save the new dataset to a NetCDF file\n",
    "    new_filename = os.path.join(output_dir, f'output_{variable_name}_{os.path.basename(file_path)}') # You can Modify if desired \n",
    "    new_ds.to_netcdf(new_filename)\n",
    "\n",
    "    # Check the new NetCDF file\n",
    "    print(f'NetCDF file saved: {new_filename}')\n",
    "    \n",
    "    # Check You can comment out this part of the code\n",
    "    print('number of original days:')\n",
    "    print(len(va))\n",
    "    print('number of inserted days:')\n",
    "    print(len(ins))\n",
    "    print('number of gregorian-based days from y0 for ny')\n",
    "    \n",
    "    print(len(pd.date_range(start='Jan 1, 1950',end='Dec 31 2100',freq='1D')))\n",
    "    print('number of days in updated dataset:')\n",
    "    #print(out.shape[0])\n",
    "    \n",
    "    # Close the original dataset\n",
    "    ds.close()\n",
    "\n",
    "# Specify the directory where your files are located\n",
    "data_dir = '/media/peterrock/Volume/Dufie_Enoch_CMIP6/ALL_MERGED_HAS_FUT/HadGEM3_GC31_LL_ssp245' #MODIFY/CHANGE HERE\n",
    "\n",
    "# Specify the output directory\n",
    "output_dir = os.path.join(data_dir, 'output_360_corrected')        # OUTPUT FOLDER : You can Modify\n",
    "\n",
    "# Use glob to select all files matching a specific pattern\n",
    "file_pattern = os.path.join(data_dir, 'PRECIP', '*.nc')     #MODIFY/CHANGE HERE, The Folder name \"PRECIP\" can be changed to folder containing datasets with 360_day calendar\n",
    "file_list = glob.glob(file_pattern)\n",
    "\n",
    "# Check if the output directory exists, remove and recreate if needed\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir)\n",
    "\n",
    "for fn in file_list:\n",
    "    # Extract the variable name in the files first before running the process function\n",
    "    variable_name = list(xr.open_dataset(fn).variables.keys())[3] # You can Modify [3], my Dataset is arranged as (time,lat,lon,pr)correspond to (0,1,2,3)\n",
    "    process_file(fn, variable_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9023f1d8-922b-4cdf-8e44-1bb3ca4a357f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82e8bf30",
   "metadata": {},
   "source": [
    "# example usage 2\n",
    "\n",
    "- CMCC-ESM2 model is 365 day, want to insert an extra day of interpolated values randomly within each leap year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad399b",
   "metadata": {},
   "source": [
    "#load net cdf of precip\n",
    "# fn = r'Precip\\\\pr_day_bccaqv2_anusplin300_cmcc_esm2_historical_ssp585_r1i1p1f1_gn_19500101_21001231_sub.nc' # For Windiws\n",
    "d = Dataset(fn)\n",
    "print('dataset calendar:')\n",
    "print(d.variables['time'].calendar) #365 day.\n",
    "\n",
    "va = d.variables['pr'][:].transpose((2,0,1)) #transposes variable from shape (col,row,time) into array (time,col,row)\n",
    "ny = int(va.shape[0]/365) #number of years\n",
    "y0 = 1950 #starting year\n",
    "\n",
    "ins = getGregorianSeeds(num_yrs=ny,y0=1950,day_cal=365) #gets a list of indices in the entire dataset where to do the interpolated insertions\n",
    "out = getInterpolatedData(arr=va,ins=ins) #makes a new array with the interpolated arrays inserted\n",
    "\n",
    "#check\n",
    "print('number of original days:')\n",
    "print(len(va))\n",
    "print('number of inserted days:')\n",
    "print(len(ins))\n",
    "print('number of gregorian-based days from y0 for ny')\n",
    "import pandas as pd #using this only for convenience of checking dates\n",
    "print(len(pd.date_range(start='Jan 1, 1950',end='Dec 31 2100',freq='1D')))\n",
    "print('number of days in updated dataset:')\n",
    "print(out.shape[0])\n",
    "\n",
    "del d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fd30d7",
   "metadata": {},
   "source": [
    "# ALTERNATIVE 2 (To example 2) FOR 365 DAY CALENDAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e5ec8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF file saved: output_365_corr/output_pr_day_CanESM5_ssp126_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_pr_day_CanESM5_ssp245_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_pr_day_CanESM5_ssp585_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tas_day_CanESM5_ssp126_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tas_day_CanESM5_ssp245_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tas_day_CanESM5_ssp585_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tasmax_day_CanESM5_ssp126_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tasmax_day_CanESM5_ssp245_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tasmax_day_CanESM5_ssp585_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tasmin_day_CanESM5_ssp126_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tasmin_day_CanESM5_ssp245_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tasmin_day_CanESM5_ssp585_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_pr_day_NorESM2-LM_ssp126_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_pr_day_NorESM2-LM_ssp245_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_pr_day_NorESM2-LM_ssp585_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_pr_day_NorESM2-MM_ssp126_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_pr_day_NorESM2-MM_ssp245_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_pr_day_NorESM2-MM_ssp585_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tas_day_NorESM2-LM_ssp126_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tas_day_NorESM2-LM_ssp245_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tas_day_NorESM2-LM_ssp585_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tas_day_NorESM2-MM_ssp126_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tas_day_NorESM2-MM_ssp245_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tas_day_NorESM2-MM_ssp585_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tasmax_day_NorESM2-LM_ssp126_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tasmax_day_NorESM2-LM_ssp245_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tasmax_day_NorESM2-LM_ssp585_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tasmax_day_NorESM2-MM_ssp126_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tasmax_day_NorESM2-MM_ssp245_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tasmax_day_NorESM2-MM_ssp585_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tasmin_day_NorESM2-LM_ssp126_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tasmin_day_NorESM2-LM_ssp245_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tasmin_day_NorESM2-LM_ssp585_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tasmin_day_NorESM2-MM_ssp126_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tasmin_day_NorESM2-MM_ssp245_HIST_merged.nc\n",
      "NetCDF file saved: output_365_corr/output_tasmin_day_NorESM2-MM_ssp585_HIST_merged.nc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "import shutil\n",
    "\n",
    "def getGregorianSeeds(num_yrs, y0, day_cal):\n",
    "    assert day_cal <= 365, 'getSeeds only configured for adding interpolated days, day_cal must be 365 or less'\n",
    "    ed = (365 - day_cal)\n",
    "    ins = []\n",
    "\n",
    "    for i, y in enumerate(range(y0, y0 + num_yrs, 1)):\n",
    "        if calendar.isleap(y):\n",
    "            b = ed + 2\n",
    "        else:\n",
    "            b = ed + 1\n",
    "        sr = np.linspace(0, day_cal, b)\n",
    "        if len(sr) == 1:\n",
    "            s = day_cal\n",
    "        else:\n",
    "            s = sr[1] - sr[0]\n",
    "        rs = np.round(np.random.rand(b - 1) * s)\n",
    "        seeds = (sr[:-1] + rs).astype(int)\n",
    "        ins.extend(seeds + day_cal * i)\n",
    "    return ins\n",
    "\n",
    "def getInterpolatedData(arr, ins):\n",
    "    r = []\n",
    "    for i, a in enumerate(arr):\n",
    "        r.append(a)\n",
    "        if i in ins:\n",
    "            r.append((arr[i] + arr[i + 1]) / 2)\n",
    "    return np.array(r)\n",
    "\n",
    "os.chdir('/media/peterrock/Volume/Dufie_Enoch_CMIP6/ALL_MERGED_HAS_FUT/HadGEM3_GC31_LL_ssp245/') # Working Directory\n",
    "data_dir = 'PRECIP_365'  # Update with the directory containing your files /Input FILES. #MODIFY/CHANGE HERE, The Folder name \"PRECIP_365\" can be changed to folder containing datasets with 365_day calendar\n",
    "\n",
    "output_dir = 'output_365_corr'  # Update with the desired output directory. MODIFY if desired\n",
    "\n",
    "# Check if the output directory exists and recreate it if necessary\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir)\n",
    "\n",
    "file_pattern = os.path.join(data_dir, '*.nc')\n",
    "file_list = glob.glob(file_pattern)\n",
    "\n",
    "for fn in file_list:\n",
    "    ds = xr.open_dataset(fn)\n",
    "\n",
    "    # Dynamically fetch the variable name\n",
    "    variable_name = list(ds.variables.keys())[3]\n",
    "\n",
    "    va = ds[variable_name].transpose('time', 'lat', 'lon').values  # Use transpose to match the order of dimensions\n",
    "    ny = int(va.shape[0] / 365)\n",
    "    y0 = 1960  # Starting year of datasets    Change DATE\n",
    "\n",
    "    ins = getGregorianSeeds(num_yrs=ny, y0=y0, day_cal=365)\n",
    "    out = getInterpolatedData(arr=va, ins=ins)\n",
    "\n",
    "    # Apply interpolation to xarray DataArray\n",
    "    time_coords = pd.date_range(start=f'Jan 1, {y0}', end=f'Dec 31 2100', freq='1D')\n",
    "    latitude_coords = ds['lat'].values\n",
    "    longitude_coords = ds['lon'].values\n",
    "\n",
    "    time_dim = xr.DataArray(time_coords, dims='time', name='time', attrs=ds['time'].attrs)\n",
    "    latitude_dim = xr.DataArray(latitude_coords, dims='lat', name='lat', attrs=ds['lat'].attrs)\n",
    "    longitude_dim = xr.DataArray(longitude_coords, dims='lon', name='lon', attrs=ds['lon'].attrs)\n",
    "\n",
    "    pr_da = xr.DataArray(out, dims=('time', 'lat', 'lon'), coords={'time': time_dim, 'lat': latitude_dim, 'lon': longitude_dim}, attrs=ds[variable_name].attrs)\n",
    "\n",
    "    new_ds = xr.Dataset({variable_name: pr_da})\n",
    "\n",
    "    # Copy global attributes from the original dataset\n",
    "    for attr in ds.attrs:\n",
    "        new_ds.attrs[attr] = ds.attrs[attr]\n",
    "\n",
    "    # Save the new dataset to a NetCDF file\n",
    "    base_filename = os.path.basename(fn)\n",
    "    new_filename = os.path.join(output_dir, f'output_{base_filename}')\n",
    "    new_ds.to_netcdf(new_filename)\n",
    "\n",
    "    # Check the new NetCDF file\n",
    "    print(f'NetCDF file saved: {new_filename}')\n",
    "\n",
    "    # Close the original dataset\n",
    "    ds.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7803b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
